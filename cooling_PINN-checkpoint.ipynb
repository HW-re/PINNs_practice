{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f695c4-07d0-4157-9661-3e4679718cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f5a9a-0a79-431f-81c7-048c315c3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna  # Import Optuna for hyperparameter tuning\n",
    "\n",
    "# Set plotting configurations\n",
    "plt.close('all')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif']  = 'Times New Roman'\n",
    "plt.rcParams['font.size']   = '18'\n",
    "plt.rcParams['figure.dpi']  = '150'\n",
    "\n",
    "# Input data for Newton's Law of Cooling\n",
    "T_amb = 27.0  # Ambient temperature\n",
    "T_o = 250.0   # Initial temperature\n",
    "k = 0.45      # Cooling constant\n",
    "\n",
    "# Define the PINN model creation function\n",
    "def create_model(trial):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(1,)))  # Input layer\n",
    "    \n",
    "    # Optuna will choose the number of hidden layers (between 1 and 5)   / Adjusted to 1 ~ 3\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "\n",
    "        # Tested neurons = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "        num_units = trial.suggest_int(f'units_layer_{i+1}', 10, 50, step=10)\n",
    "        # activation = trial.suggest_categorical(f'activation_layer_{i+1}', ['relu', 'tanh', 'sigmoid'])\n",
    "        activation = trial.suggest_categorical(f'activation_layer_{i+1}', ['tanh'])\n",
    "        model.add(tf.keras.layers.Dense(num_units, activation=activation))\n",
    "\n",
    "    # Output layer (predicting temperature)\n",
    "    model.add(tf.keras.layers.Dense(1, dtype='float32'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "def loss(model, t, t_ic, T_ic):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        T_pred = model(t, training=True)  # Use model directly\n",
    "        \n",
    "    T_t = tape.gradient(T_pred, t)\n",
    "\n",
    "    f = T_t - k * (T_amb - T_pred)  # PDE residual\n",
    "    loss_pde = tf.reduce_mean(tf.square(f))  # Enforcing PDE satisfaction\n",
    "    T_ic_pred = model(t_ic, training=True)\n",
    "    loss_ic = tf.reduce_mean(tf.square(T_ic - T_ic_pred))  # Initial condition loss\n",
    "\n",
    "    return loss_pde + loss_ic  # Total loss\n",
    "\n",
    "\n",
    "\n",
    "# Define the training step\n",
    "def train_step(model, t, t_ic, T_ic, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, t, t_ic, T_ic)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Optuna objective function (used to optimize the model architecture)\n",
    "def objective(trial):\n",
    "    model = create_model(trial)  # Create model with suggested architecture\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Use Adam optimizer\n",
    "\n",
    "    # Generate training data\n",
    "    t_train = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    t_train = tf.convert_to_tensor(t_train, dtype=tf.float32)\n",
    "\n",
    "    # Initial condition data\n",
    "    t_ic = np.array([[0.0]], dtype=np.float32)\n",
    "    T_ic = np.array([[T_o]], dtype=np.float32)\n",
    "    t_ic = tf.convert_to_tensor(t_ic, dtype=tf.float32)\n",
    "    T_ic = tf.convert_to_tensor(T_ic, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    \n",
    "    # Train the model for a fixed number of epochs\n",
    "    max_epochs = 15000         # Limit training to avoid excessive computation\n",
    "    loss_threshold = 1e-3     # Stop training early if loss is low enough\n",
    "    min_loss_reduction = 0.1  # Stop if loss decreases by less than 10%\n",
    "\n",
    "    prev_loss = float(\"inf\")  # Initialize previous loss as inifinity\n",
    "\n",
    "    \n",
    "    # ** Early stopping based on loss_value is less than loss_threshold\n",
    "    for epoch in range(max_epochs):\n",
    "        loss_value = train_step(model, t_train, t_ic, T_ic, optimizer)\n",
    "\n",
    "        if loss_value.numpy() < loss_threshold:\n",
    "            print(f\"Early stopping because loss below the loss_threshold at epoch {epoch}\")\n",
    "            break  # Early stopping\n",
    "\n",
    "        if prev_loss != float(\"inf\"):   # Avoid checking on the first epoch\n",
    "            loss_reduction = abs(prev_loss - loss_value) / prev_loss\n",
    "            if loss_reduction < min_loss_reduction:\n",
    "                print(f\"Early stopping because loss reduction is below 10% at epoch {epoch}\")\n",
    "                break    # Early stopping\n",
    "\n",
    "        prev_loss = loss_value     # Update previous loss for next iteration\n",
    "    \n",
    "    return loss_value.numpy()  # Optuna minimizes this loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the hyperparameter optimization using Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Faster search: use a Bayesian Optimizer\n",
    "# TPESampler: prioritizes promising architectures instead of randomly guessing.\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=10)  # Run 20 trials to find the best architecture\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparams = study.best_params\n",
    "print(\"\\n Best Architecture Found by Optuna:\")\n",
    "print(best_hyperparams)\n",
    "\n",
    "# Create and train the best model\n",
    "best_model = create_model(optuna.trial.FixedTrial(best_hyperparams))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Train the best model again\n",
    "t_train = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "t_train = tf.convert_to_tensor(t_train, dtype=tf.float32)\n",
    "t_ic = np.array([[0.0]], dtype=np.float32)\n",
    "T_ic = np.array([[T_o]], dtype=np.float32)\n",
    "t_ic = tf.convert_to_tensor(t_ic, dtype=tf.float32)\n",
    "T_ic = tf.convert_to_tensor(T_ic, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate test data for prediction\n",
    "t_test = np.linspace(0, 10, 1000).reshape(-1, 1)\n",
    "t_test = tf.convert_to_tensor(t_test, dtype=tf.float32)\n",
    "T_pred = best_model(t_test, training=False).numpy()\n",
    "\n",
    "# Compute the analytical solution for comparison\n",
    "T_true = T_amb + (T_o - T_amb) * np.exp(-k * t_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(t_test.numpy(), T_true, label=\"Analytical Solution\", linestyle='dashed')\n",
    "plt.plot(t_test.numpy(), T_pred, label=\"PINN Prediction\", alpha=0.8)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Temperature (Â°C)\")\n",
    "plt.legend()\n",
    "plt.title(\"PINN vs Analytical Solution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11158e72-8253-4d21-8bcb-87dcea95e407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
